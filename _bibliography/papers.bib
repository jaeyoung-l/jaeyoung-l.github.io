---
---
@misc{chung2025dontlookoncemultimodal,
    abbr={Preprint},
    title={Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation},
    abstract={We present v1, a lightweight extension to Multimodal Large Language Models(MLLMs) that enables selective visual revisitation during inference. While current MLLMs typically consume visual input only once and reason purely over internal memory, v1 introduces a simple point-and-copy mechanism that allows the model to dynamically retrieve relevant image regions throughout the reasoning process. This mechanism augments existing architectures with minimal modifications, enabling contextual access to visual tokens based on the model’s evolving hypotheses. To train this capability, we construct v1g, a dataset of 300K multimodal reasoning traces with interleaved visual grounding annotations. Experiments on three multimodal mathematical reasoning benchmarks—MathVista, MathVision, and MathVerse—demonstrate that v1 consistently improves performance over comparable baselines, particularly on tasks requiring fine-grained visual reference and multi-step reasoning. Our results suggest that dynamic visual access is a promising direction for enhancing grounded multimodal reasoning. Code, models, and data will be released to support future research.},
    author={Jiwan Chung and Junhyeok Kim and Siyeol Kim and Jaeyoung Lee and Minsoo Kim and Youngjae Yu},
    journal={arXiv preprint arXiv:2505.18842},
    year={2025},
    arxiv={2505.18842},
    selected={true}
}


@article{lee2024open-fact-verifier,
    abbr={EMNLP},
    title={How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models},
    abstract={Given the growing influx of misinformation across news and social media, there is a critical need for systems that can provide effective real-time verification of news claims. Large language or multimodal model based verification has been proposed to scale up online policing mechanisms for mitigating spread of false and harmful content. While these can potentially reduce burden on human fact-checkers, such efforts may be hampered by foundation model training data becoming outdated. In this work, we test the limits of improving foundation model performance without continual up- dating through an initial study of knowledge transfer using either existing intra- and inter- domain benchmarks or explanations generated from large language models (LLMs). <br/><br/> We evaluate on 12 public benchmarks for fact-checking and misinformation detection as well as two other tasks relevant to content moderation - toxicity and stance detection. Our results on two recent multi-modal fact-checking benchmarks, Mocheg and Fakeddit, indicate that knowledge transfer strategies can improve Fakeddit performance over the state-of-the-art by up to 1.7% and Mocheg performance by up to 2.9%.},
    author = {Jaeyoung Lee and Ximing Lu and Jack Hessel and Faeze Brahman and Youngjae Yu and Yonatan Bisk and Yejin Choi and Saadia Gabriel},
    journal={Findings of the Conference on Empirical Methods in Natural Language Processing},
    year={2024},
    arxiv={2407.00369},
    selected={true}
}


@article{Song_2022_CVPR,
    abbr={CVPR Workshops},
    title={FS-NCSR: Increasing Diversity of the Super-Resolution Space via Frequency Separation and Noise-Conditioned Normalizing Flow},
    journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    author={Kiung Song and Dongseok Shim and Kangwook Kim and Jaeyoung Lee and Younggeun Kim},
    year={2022},
    selected={true}
}
